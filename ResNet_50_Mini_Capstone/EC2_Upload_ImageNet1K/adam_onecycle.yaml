optimizer: adam
lr_scheduler: onecycle
learning_rate: 0.003
weight_decay: 0.0001
epochs: 90
batch_size: 256
steps_per_epoch: 100
onecycle_pct_start: 0.3
onecycle_anneal: cos
onecycle_final_div_factor: 10000
save_every_n_batches: 500
log_interval: 100
use_fp16: true