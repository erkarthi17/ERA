# Fine-tuning configuration for ResNet-50
optimizer: adamw               # Switched to AdamW for better weight decay handling
lr_scheduler: onecycle
learning_rate: 0.0003          # CRITICAL: 10x lower max learning rate for fine-tuning
weight_decay: 0.0001
epochs: 25                     # A new, shorter training schedule of 25 epochs

# --- Regularization Improvements ---
label_smoothing: 0.1
mixup_alpha: 0.2

# --- OneCycleLR specific settings ---
# These are fine to keep as they are relative to the new `epochs`
onecycle_pct_start: 0.3
onecycle_anneal: cos
onecycle_final_div_factor: 10000

# --- Performance & Logging ---
compile_model: true            # Enable torch.compile for a speed boost
batch_size: 256
save_every_n_batches: 500
log_interval: 100
use_fp16: true