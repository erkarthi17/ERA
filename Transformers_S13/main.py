# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dWD87DyRvo7Mx1D8YxMQfQdEOLAi8Cox

âœ… ðŸ“˜ COMPLETE GOOGLE COLAB NOTEBOOK â€” ALL CELLS IN ORDER

Install Dependencies
"""

!pip install transformers accelerate datasets sentencepiece pyyaml tqdm tensorboard
import torch
torch.__version__

"""Upload Model & Config File"""

from google.colab import files

print("Please upload SmolLm3.py and config_smollm2_135M.yaml")
uploaded = files.upload()

print("Uploaded files:", list(uploaded.keys()))

"""Load Model Config & Architecture"""

import yaml
from SmolLm3 import LlamaModel
import torch

with open("SmolLm3_Config.yaml") as f:
    full_cfg = yaml.safe_load(f)

model_cfg = full_cfg["model"]
model = LlamaModel(model_cfg)

print("Model loaded. Total params:", sum(p.numel() for p in model.parameters()))

"""Load HF SmolLm3-135M"""

from transformers import AutoModelForCausalLM, AutoTokenizer

HF_MODEL = "HuggingFaceTB/SmolLM-135M"

tokenizer = AutoTokenizer.from_pretrained(HF_MODEL)
hf_model = AutoModelForCausalLM.from_pretrained(HF_MODEL)

print("HF model params:", sum(p.numel() for p in hf_model.parameters()))

"""Confirm Layer Count Match"""

print("HF num_hidden_layers:", hf_model.config.num_hidden_layers)
print("Local num_hidden_layers (from YAML):", model_cfg["model_config"]["num_hidden_layers"])
print("Local actual layers (len(model.layers)):", len(model.layers))

"""Map HF Keys & Local Keys"""

# 1) Get raw state dicts
local_state = model.state_dict()
hf_state = hf_model.state_dict()

# 2) Map HF keys -> local keys by stripping "model."
def map_hf_to_local(hf_state):
    mapped = {}
    for k, v in hf_state.items():
        if k.startswith("model."):
            new_k = k.replace("model.", "")
        else:
            new_k = k
        mapped[new_k] = v
    return mapped

mapped_state = map_hf_to_local(hf_state)

# 3) Compare mapped keys vs local keys (THIS is the relevant diff)
local_keys = set(local_state.keys())
mapped_keys = set(mapped_state.keys())

missing_after = mapped_keys - local_keys       # in checkpoint, not in model
extra_after   = local_keys - mapped_keys       # in model, not in checkpoint

print("Missing AFTER mapping:", list(missing_after)[:50])
print("Extra AFTER mapping:", list(extra_after)[:50])

"""Load HF Weight into Model"""

model.load_state_dict(mapped_state, strict=False)
print("Loaded mapped HF weights into local model.")

"""Validate Weigth Swap (Generation Test)"""

prompt = "The capital of France is"
inputs = tokenizer(prompt, return_tensors="pt")

hf_out = hf_model.generate(**inputs, max_new_tokens=20)

local_out = model.generate(
    inputs["input_ids"],
    max_new_tokens=20,
    context_length=128,
    eos_token=tokenizer.eos_token_id,
    device="cpu"
)

print("HF:   ", tokenizer.decode(hf_out[0],   skip_special_tokens=True))
print("Local:", tokenizer.decode(local_out[0], skip_special_tokens=True))

"""Load Dataset"""

import torch
from datasets import load_dataset

# Load text from input.txt
with open('input.txt', 'r') as f:
    input_text = f.read()

# Split the text into smaller chunks if necessary for training or use as one large text
# For simplicity, let's treat the entire content as one 'document' or split by newlines
train_texts = input_text.split('\n\n') # Split by double newlines for paragraph-like chunks
# Filter out empty strings that might result from splitting
train_texts = [text for text in train_texts if text.strip()]

# Fix: Assign a padding token to the tokenizer
tokenizer.pad_token = tokenizer.eos_token

def encode(text):
    return tokenizer(text, truncation=True, max_length=128, padding="max_length")["input_ids"]

train_ids = [encode(t) for t in train_texts]
train_tensor = torch.tensor(train_ids)

"""Setup TensorBoard Logging"""

from torch.utils.tensorboard import SummaryWriter
import datetime

log_dir = "runs/smollm_" + datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
writer = SummaryWriter(log_dir)

print("Logging to:", log_dir)

"""Start TensorBoard"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir runs

"""Log Text Generation (Utility)"""

def log_generation(step, model, tokenizer, device, context_len):
    prompt = "Once upon a time,"
    ids = tokenizer(prompt, return_tensors="pt")["input_ids"].to(device)

    out = model.generate(
        ids,
        max_new_tokens=80,
        context_length=context_len,
        eos_token=tokenizer.eos_token_id,
        device=device
    )

    text = tokenizer.decode(out[0], skip_special_tokens=True)
    writer.add_text("generation/sample", text, step)

    print(f"\n=== Generation @ step {step} ===")
    print(text)
    print("================================\n")

"""Training Loop (5000 Steps) with Tensor Board Logging"""

model = model.to(device) # Move the model to the specified device (GPU)

for epoch in range(999999):
    for i in range(0, len(train_tensor), batch_size):
        batch = train_tensor[i:i+batch_size].to(device)

        # FIX: make x and y contiguous
        x = batch[:, :-1].contiguous()
        y = batch[:, 1:].contiguous()

        logits, loss = model(x, y)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        step += 1
        progress.update(1)

        writer.add_scalar("loss/train", loss.item(), step)

        if step % 100 == 0: # Print loss every 100 steps
            print(f"Step: {step}, Train Loss: {loss.item():.4f}")

        if step % 500 == 0:
            log_generation(step, model, tokenizer, device, context_len)

        if step >= max_steps:
            break
    if step >= max_steps:
        break

progress.close()
torch.save(model.state_dict(), "checkpoint_5000.pt")
print("Saved checkpoint_5000.pt")

"""Resuming Training for 50 More stpes from Check Point"""

model.load_state_dict(torch.load("checkpoint_5000.pt"))
model = model.to(device)

extra_steps = 50
step = 0

progress = tqdm(total=extra_steps, desc="Resume Training", ncols=120)

for epoch in range(999999):
    for i in range(0, len(train_tensor), batch_size):
        batch = train_tensor[i:i+batch_size].to(device)

        # FIX: ensure x and y are contiguous before passing to model
        x = batch[:, :-1].contiguous()
        y = batch[:, 1:].contiguous()

        logits, loss = model(x, y)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        step += 1
        progress.update(1)

        writer.add_scalar("loss/resume", loss.item(), step)

        if step % 10 == 0: # Print loss every 10 steps for the smaller resume loop
            print(f"Step: {step}, Resume Loss: {loss.item():.4f}")

        if step >= extra_steps:
            break
    if step >= extra_steps:
        break

progress.close()
torch.save(model.state_dict(), "checkpoint_5050.pt")
print("Saved checkpoint_5050.pt")

"""Final Log Generation after 5050 Steps"""

log_generation(5050, model, tokenizer, device, context_len)