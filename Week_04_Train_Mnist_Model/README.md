# ğŸ–¤ MNIST Classification with PyTorch

This project trains a **Convolutional Neural Network (CNN)** on the classic [MNIST handwritten digits dataset](http://yann.lecun.com/exdb/mnist/).  
It includes two architectures:  

- **`Net`** â€“ A deep CNN ( > 1M parameters )  
- **`TinyNet`** â€“ A lightweight CNN with **22,514 trainable parameters**  
  - Specially designed to stay **under 25k parameters** while still reaching **>95% accuracy in the first epoch** ğŸš€

---

## ğŸ“‚ Project Structure

â”œâ”€â”€ main.py # Training script
â”œâ”€â”€ requirements.txt # Dependencies
â”œâ”€â”€ README.md # Project documentation
â”œâ”€â”€ results/ # Saved plots & model summaries
â”‚ â”œâ”€â”€ training_curves.png
â”‚ â””â”€â”€ model_summary.txt
â””â”€â”€ data/ # MNIST dataset (auto-downloaded)

## âš¡ Installation

```bash
# Clone this repo
git clone [https://github.com/your-username/mnist-pytorch.git](https://github.com/erkarthi17/ERA.git)
cd mnist-pytorch

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows 

# Install dependencies
pip install -r requirements.txt
```

## â–¶ï¸ Usage

Train with default settings:
----------------------------
   
   python main.py

Train with custom arguments:
----------------------------

   python main.py --epochs 5 --batch-size 256 --lr 0.01

Available arguments:
--------------------

--batch-size : Batch size (default = 512)

--epochs : Number of training epochs (default = 20)

--lr : Learning rate (default = 0.001)

--step-size : Scheduler step size (default = 15)

--gamma : Scheduler decay factor (default = 0.1)

--no-cuda : Force CPU training even if GPU is available

## ğŸ—ï¸ TinyNet Architecture (<25k params)

The TinyNet architecture was carefully designed to balance efficiency and accuracy:

| Layer               | Output Shape      | Params |
| ------------------- | ----------------- | ------ |
| Conv2d (1â†’20) + BN  | 20 Ã— 28 Ã— 28      | 240    |
| MaxPool2d           | 20 Ã— 14 Ã— 14      | 0      |
| Conv2d (20â†’28) + BN | 28 Ã— 14 Ã— 14      | 5,104  |
| MaxPool2d           | 28 Ã— 7 Ã— 7        | 0      |
| Conv2d (28â†’64) + BN | 64 Ã— 7 Ã— 7        | 16,320 |
| AdaptiveAvgPool2d   | 64 Ã— 1 Ã— 1        | 0      |
| Fully Connected     | 10                | 650    |

| **Total Params**    | **22,514** âœ… <25k 

## ğŸ” How it works

Conv1 + BN + Pooling â€“ Extracts low-level features (edges, strokes).

Conv2 + BN + Pooling â€“ Captures mid-level patterns (digit parts).

Conv3 + BN â€“ Learns high-level digit representations.

Global Average Pooling (GAP) â€“ Reduces feature maps to a compact vector.

FC Layer â€“ Classifies into 10 digit classes (0â€“9).

ğŸ“Š Results
-----------

   python main.py --epochs 5 --batch-size 256 --lr 0.01

   Test Accuracy after 1st epoch: ~95â€“96% âœ…

   Final Accuracy (5 epochs): >98% ğŸ¯

Training & evaluation curves are saved in results/training_curves.png.
