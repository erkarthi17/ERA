{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d62d3e7a",
   "metadata": {},
   "source": [
    "# Notes & Tips\n",
    "\n",
    "- Use `--pretrained` (or load with `GPT.from_pretrained('gpt2')`) for much faster convergence to low loss.\n",
    "- For best performance on Colab T4: use mixed precision (AMP) and increase batch size and gradient accumulation to reach an effective large batch size.\n",
    "- Save checkpoints to Drive to avoid losing progress when the runtime disconnects.\n",
    "- To get to a validation loss < 0.1 quickly, you can fine-tune a pretrained model on a small dataset (but that will overfit; use a validation set to monitor generalization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5fa98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation example (load best checkpoint and generate)\n",
    "from pathlib import Path\n",
    "ckpt_path = Path(DRIVE_CKPT_DIR) / 'ckpt_best.pth'\n",
    "if ckpt_path.exists():\n",
    "    print('Loading checkpoint', ckpt_path)\n",
    "    load_checkpoint(str(ckpt_path), model)\n",
    "else:\n",
    "    print('Best checkpoint not found, using current model state')\n",
    "\n",
    "# initialize generation sequence from training batch\n",
    "x, _ = train_loader.next_batch(split='train')\n",
    "# take the first example in batch as prompt\n",
    "x = x.to(device)[:1]\n",
    "\n",
    "num_return_sequences = 1\n",
    "max_length = 100\n",
    "with torch.no_grad():\n",
    "    while x.size(1) < max_length:\n",
    "        logits, _ = model(x)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        ix = torch.multinomial(topk_probs, 1)\n",
    "        xcol = torch.gather(topk_indices, -1, ix)\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "tokens = x[0].tolist()\n",
    "print('Generated text:\\n', train_loader.enc.decode(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de94042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: quick run (pretrained) - adjust hyperparameters as needed\n",
    "# Make sure you have ./input.txt in the notebook working directory (upload or copy from Drive)\n",
    "\n",
    "# Example settings\n",
    "B = 8\n",
    "T = 128\n",
    "max_steps = 2000  # increase for real training\n",
    "lr = 3e-4\n",
    "accum_steps = 1\n",
    "\n",
    "# Create data loader (point path to your input.txt)\n",
    "!ls -lh input.txt || echo 'input.txt not found; place it in the notebook dir or copy from Drive'\n",
    "train_loader = DataLoaderLite(B=B, T=T, path='input.txt', val_fraction=0.05)\n",
    "\n",
    "# Load pretrained GPT-2 into our model class (recommended)\n",
    "model = GPT.from_pretrained('gpt2')\n",
    "model.to(device)\n",
    "\n",
    "# Kick off training (saves checkpoints to DRIVE_CKPT_DIR)\n",
    "train_simple(model, train_loader, device, ckpt_dir=DRIVE_CKPT_DIR, batch_size=B, seq_len=T, max_steps=max_steps, lr=lr, weight_decay=0.1, accum_steps=accum_steps, val_interval=200, save_interval=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca50e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training utilities: evaluate, checkpoint saving/loading, train\n",
    "import types\n",
    "\n",
    "def evaluate(model, data_loader, device, use_amp=False, max_batches=None):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        num = 0\n",
    "        while True:\n",
    "            if max_batches is not None and num >= max_batches:\n",
    "                break\n",
    "            try:\n",
    "                x, y = data_loader.next_batch(split='val')\n",
    "            except Exception:\n",
    "                break\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                _, loss = model(x, y)\n",
    "            losses.append(loss.item())\n",
    "            num += 1\n",
    "            if data_loader.val_position == 0:\n",
    "                break\n",
    "    model.train()\n",
    "    return float(sum(losses) / max(1, len(losses))) if losses else float('inf')\n",
    "\n",
    "\n",
    "def save_checkpoint(path, model, optimizer, scaler, step, best_val):\n",
    "    ckpt = {\n",
    "        'model_state': model.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict() if optimizer is not None else None,\n",
    "        'scaler_state': scaler.state_dict() if scaler is not None else None,\n",
    "        'step': step,\n",
    "        'best_val': best_val,\n",
    "    }\n",
    "    torch.save(ckpt, path)\n",
    "    print('Saved checkpoint to', path)\n",
    "\n",
    "\n",
    "def load_checkpoint(path, model, optimizer=None, scaler=None):\n",
    "    ckpt = torch.load(path, map_location='cpu')\n",
    "    model.load_state_dict(ckpt['model_state'])\n",
    "    if optimizer is not None and ckpt.get('optimizer_state') is not None:\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state'])\n",
    "    if scaler is not None and ckpt.get('scaler_state') is not None:\n",
    "        scaler.load_state_dict(ckpt['scaler_state'])\n",
    "    return ckpt.get('step', 0), ckpt.get('best_val', float('inf'))\n",
    "\n",
    "\n",
    "def train_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    device,\n",
    "    ckpt_dir,\n",
    "    batch_size=8,\n",
    "    seq_len=128,\n",
    "    max_steps=2000,\n",
    "    lr=3e-4,\n",
    "    weight_decay=0.1,\n",
    "    accum_steps=1,\n",
    "    val_interval=200,\n",
    "    save_interval=500,\n",
    "    target_loss=0.099999,\n",
    "    use_pretrained=False,\n",
    "):\n",
    "    use_amp = (device.type == 'cuda')\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    best_val = float('inf')\n",
    "    pbar = tqdm(total=max_steps)\n",
    "    step = 0\n",
    "    while step < max_steps:\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = 0.0\n",
    "        for _ in range(accum_steps):\n",
    "            x, y = train_loader.next_batch(split='train')\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                _, loss = model(x, y)\n",
    "            loss = loss / accum_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            total_loss += loss.item()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        step += 1\n",
    "        pbar.update(1)\n",
    "        if step % 10 == 0:\n",
    "            pbar.set_postfix({'loss': f\"{total_loss:.6f}\", 'step': step})\n",
    "        if step % val_interval == 0:\n",
    "            val_loss = evaluate(model, train_loader, device, use_amp, max_batches=32)\n",
    "            print(f\"[val] step {step} loss {val_loss:.6f}\")\n",
    "            if val_loss < best_val:\n",
    "                best_val = val_loss\n",
    "                save_checkpoint(os.path.join(ckpt_dir, 'ckpt_best.pth'), model, optimizer, scaler, step, best_val)\n",
    "            if val_loss < target_loss:\n",
    "                print(f\"Target val loss {target_loss} reached at step {step} (val {val_loss:.6f}).\")\n",
    "                break\n",
    "        if step % save_interval == 0:\n",
    "            save_checkpoint(os.path.join(ckpt_dir, f'ckpt_step{step}.pth'), model, optimizer, scaler, step, best_val)\n",
    "    pbar.close()\n",
    "    save_checkpoint(os.path.join(ckpt_dir, 'ckpt_final.pth'), model, optimizer, scaler, step, best_val)\n",
    "    print('Training finished. Best val:', best_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e3827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader (loads text and tokenizes with tiktoken)\n",
    "import tiktoken\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, path='input.txt', val_fraction=0.05):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        self.enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = self.enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        self.val_split = int(val_fraction * len(self.tokens))\n",
    "        if self.val_split < (B * T + 1):\n",
    "            self.val_split = 0\n",
    "        if self.val_split > 0:\n",
    "            self.val_tokens = self.tokens[-self.val_split:]\n",
    "            self.train_tokens = self.tokens[:-self.val_split]\n",
    "        else:\n",
    "            self.val_tokens = torch.tensor([], dtype=self.tokens.dtype)\n",
    "            self.train_tokens = self.tokens\n",
    "        print(f'loaded {len(self.tokens)} tokens (train={len(self.train_tokens)}, val={len(self.val_tokens)})')\n",
    "        print(f'1 epoch = {len(self.train_tokens) // (B * T)} batches')\n",
    "        self.current_position = 0\n",
    "        self.val_position = 0\n",
    "\n",
    "    def next_batch(self, split='train'):\n",
    "        B, T = self.B, self.T\n",
    "        if split == 'train' or len(self.val_tokens) == 0:\n",
    "            tokens = self.train_tokens\n",
    "            pos = self.current_position\n",
    "            if pos + (B * T + 1) > len(tokens):\n",
    "                pos = 0\n",
    "            buf = tokens[pos: pos + B * T + 1]\n",
    "            x = (buf[:-1]).view(B, T)\n",
    "            y = (buf[1:]).view(B, T)\n",
    "            self.current_position = pos + B * T\n",
    "            if self.current_position + (B * T + 1) > len(tokens):\n",
    "                self.current_position = 0\n",
    "            return x, y\n",
    "        else:\n",
    "            tokens = self.val_tokens\n",
    "            pos = self.val_position\n",
    "            if pos + (B * T + 1) > len(tokens):\n",
    "                pos = 0\n",
    "            buf = tokens[pos: pos + B * T + 1]\n",
    "            x = (buf[:-1]).view(B, T)\n",
    "            y = (buf[1:]).view(B, T)\n",
    "            self.val_position = pos + B * T\n",
    "            if self.val_position + (B * T + 1) > len(tokens):\n",
    "                self.val_position = 0\n",
    "            return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f58950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT model and config\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type='gpt2'):\n",
    "        print('Loading pretrained from HF:', model_type)\n",
    "        config_map = {\n",
    "            'gpt2': dict(n_layer=12, n_head=12, n_embd=768),\n",
    "            'gpt2-medium': dict(n_layer=24, n_head=16, n_embd=1024),\n",
    "            'gpt2-large': dict(n_layer=36, n_head=20, n_embd=1280),\n",
    "            'gpt2-xl': dict(n_layer=48, n_head=25, n_embd=1600),\n",
    "        }\n",
    "        cfg = GPTConfig(**config_map[model_type])\n",
    "        model = GPT(cfg)\n",
    "        # load HF weights and copy\n",
    "        hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd = model.state_dict()\n",
    "        sd_hf = hf.state_dict()\n",
    "        skip = [k for k in sd.keys() if k.endswith('.attn.bias')]\n",
    "        sd_keys = [k for k in sd.keys() if k not in skip]\n",
    "        sd_hf_keys = [k for k in sd_hf.keys() if not k.endswith('.attn.masked_bias') and not k.endswith('.attn.bias')]\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        assert len(sd_keys) == len(sd_hf_keys), 'mismatched keys when loading HF model'\n",
    "        for k_hf, k in zip(sd_hf_keys, sd_keys):\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                sd[k].copy_(sd_hf[k_hf].t())\n",
    "            else:\n",
    "                sd[k].copy_(sd_hf[k_hf])\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2576e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture (CausalSelfAttention, MLP, Block)\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d2c7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration and seeds\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "seed = 1337\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload input.txt from local machine (optional)\n",
    "# Run this cell and choose your file; it will be stored as ./input.txt\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "if 'input.txt' in uploaded:\n",
    "    print('input.txt uploaded')\n",
    "else:\n",
    "    # if user uploaded under a different name, copy it\n",
    "    for fname in uploaded:\n",
    "        print('uploaded', fname)\n",
    "    # optionally, if you named file differently, copy it\n",
    "    # e.g., files.upload() -> uploaded['myfile.txt'] ; then rename or copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d900a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and prepare checkpoint directory\n",
    "from google.colab import drive, files\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "DRIVE_CKPT_DIR = '/content/drive/MyDrive/gpt_ckpts'\n",
    "os.makedirs(DRIVE_CKPT_DIR, exist_ok=True)\n",
    "print('Checkpoints will be saved to:', DRIVE_CKPT_DIR)\n",
    "\n",
    "print('\\nIf you want to upload a local input.txt, run the cell below to upload and copy it to ./input.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e33ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and install dependencies\n",
    "!pip install -q transformers tiktoken tqdm accelerate\n",
    "\n",
    "import torch\n",
    "print('torch', torch.__version__, 'cuda available:', torch.cuda.is_available())\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695be8a8",
   "metadata": {},
   "source": [
    "# Colab: Train minGPT (GPT-2 small / 124M) on a T4 GPU\n",
    "\n",
    "This notebook prepares the environment, mounts Google Drive for checkpoints, loads data, and trains a decoder-only model based on the `main.py` code in this repo.\n",
    "\n",
    "Instructions:\n",
    "- Runtime → Change runtime type → GPU (T4 recommended)\n",
    "- Mount Drive (below) to save checkpoints and load your `input.txt` dataset\n",
    "- Consider using `--pretrained` (pretrained GPT-2) for much faster convergence\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
